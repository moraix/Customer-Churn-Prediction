{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22d8691e-5895-46e5-af8f-2a39d4b6ab7b",
   "metadata": {},
   "source": [
    "<center><h1>Predicting Customer Churn Using Deep Learning</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cb5edb-178c-4709-af3a-eea184da4645",
   "metadata": {},
   "source": [
    "<h2>Introduction</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f86f1a-6f28-48f9-8be6-0e8b74ee8f4d",
   "metadata": {},
   "source": [
    "In this project, we develop a deep learning model to predict customer churn. For this manner, we will use the <a href=\"https://www.kaggle.com/datasets/barun2104/telecom-churn\">Customer Churn</a> dataset from Kaggle. This data set contains customer level information for a telecom company. Various attributes related to the services used are recorded for each customer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc683c8d-4dc3-4161-858f-74ff3a8aeaf8",
   "metadata": {},
   "source": [
    "<h2>Download / Load Dataset</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73099a71-cde6-48a3-a0fa-1daf2fd7183f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dca5140-edb7-4c4b-bf59-c22820ff5c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv ('/kaggle/input/telecom-churn/telecom_churn.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757bbf49-92b0-4e72-a342-76b0ed729e16",
   "metadata": {},
   "source": [
    "Each row of this dataset, represents a customer and each column contains attributes related to customer. The “Churn” column is the target variable. For more information about columns, you can visit the dataset's page in Kaggle website."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b82ece5-30cc-4ccb-95fa-023ff84b5de3",
   "metadata": {},
   "source": [
    "<h2>Preprocessing</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dec12df-4fb6-41be-ab9d-8d58bed56b86",
   "metadata": {},
   "source": [
    "One of the key steps in preprocessing for this type of project is ensuring that the data is balanced. In the case of churn prediction, this means that the number of instances for each class (churn = 1, no churn = 0) should be equal or at least similar. Imbalanced data can negatively impact the training process, leading to a biased model that favors the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f94557-038a-4d5d-96cc-7c93856b72fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_counts = df['Churn'].value_counts()\n",
    "print(churn_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d106ee49-08f4-433d-a276-6459f593dbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_percentage = df['Churn'].value_counts(normalize=True) * 100\n",
    "print(churn_percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d151bb6-ac63-4d1f-a335-ae8748633c0a",
   "metadata": {},
   "source": [
    "As you can see, the number of instances for the churn class '0' is significantly higher than for class '1', making the dataset clearly imbalanced. However, before addressing this imbalance, let's first split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f58074-7121-4f14-b0b2-63f34b5b72b6",
   "metadata": {},
   "source": [
    "<h2>Split Data into Train and Test Sets</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da9fbd6-7c3e-4075-a370-e4b357b6b80a",
   "metadata": {},
   "source": [
    "As mentioned before, the \"Churn\" column is our target value. So, we can split the data like this:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263b035b-27e8-431d-8d59-f9f3a6f37c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(columns=['Churn'])\n",
    "y = df['Churn']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15d7581-9d7b-423a-b0e1-56a33172beec",
   "metadata": {},
   "source": [
    "<h2>Balancing the Data</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d593c3-627e-45e0-b183-de7be207c4ae",
   "metadata": {},
   "source": [
    "Now, we can balance the data. There are multiple methods to achieve this, but given the nature of our dataset, we will use the SMOTE (Synthetic Minority Over-sampling Technique) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41af3bb0-2cd8-4085-95ee-03b654dc103e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcbe0e0-c415-48a3-9094-a03f1f93a18b",
   "metadata": {},
   "source": [
    "Now, let's see the result of using SMOTE method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c11391-b9cb-43b6-8c31-ad66d15a1320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Before SMOTE:\")\n",
    "print(\"\\nBefore SMOTE:\")\n",
    "print(y_train.value_counts(normalize=True) * 100)  # Original class distribution\n",
    "\n",
    "# print(\"After SMOTE:\")\n",
    "print(\"\\nAfter SMOTE:\")\n",
    "print(y_train_resampled.value_counts(normalize=True) * 100)  # Balanced class distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3304745d-f7b5-4e85-9590-24c0cf94ad67",
   "metadata": {},
   "source": [
    "As you can see, the number of instances in each churn class is now equal. However, it's important to note that using the SMOTE method can introduce some challenges, such as:\n",
    "\n",
    "<li>Synthetic Data Artifacts – SMOTE generates synthetic samples based on existing data, which may not always represent real-world patterns accurately.</li>\n",
    "<li>Overfitting – The model may learn patterns specific to the synthetic data rather than generalizing well to new, unseen data.</li>\n",
    "<li>Impact on Feature Distribution – SMOTE may slightly alter the distribution of certain features, especially if the dataset is highly imbalanced.</li>\n",
    "<br>\n",
    "To mitigate these issues, it's important to carefully evaluate the model's performance and consider alternative techniques if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a13b423-c0ff-4b6c-a0ee-f455e03cfd9d",
   "metadata": {},
   "source": [
    "<h2>Feature Scaling</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d89de27-b851-4102-8ede-d4f1381a9059",
   "metadata": {},
   "source": [
    "After balancing the data, the next crucial step is feature scaling. Scaling ensures that all numerical features have a similar range, preventing models from being biased toward features with larger values.\n",
    "\n",
    "In our dataset, some features have values ranging from 0 to 1, while others have much larger ranges, such as 110, 256.2, or 13.7. If left unscaled, this difference can negatively impact the model's performance, as features with larger magnitudes may dominate the learning process.\n",
    "\n",
    "To address this, we use Standard Scaling, which transforms the features to have a mean of 0 and a standard deviation of 1. This method helps deep learning models converge faster and improves overall performance.\n",
    "\n",
    "Since scaling should be based only on training data to prevent data leakage, we fit the scaler on the training set and apply the transformation to both the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecea8b8-1e65-434c-b3db-913b10b4ee7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train_resampled)  # Apply scaling on resampled training data\n",
    "X_test_scaled = scaler.transform(X_test)  # Apply scaling on test data (don't fit on test)\n",
    "\n",
    "print(X_train_scaled[:5])  # Print first 5 rows of scaled training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce5ea2e-2f62-4eae-8af5-4949a1ed137a",
   "metadata": {},
   "source": [
    "<h2>Build the Deep Learning Model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e6762d-509a-4371-909c-6363e7f2650f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a58cf4-5946-4fd5-9084-b36b703fb48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(128, input_dim=X_train_scaled.shape[1], activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3288cdba-b610-4179-b517-34c8080032ea",
   "metadata": {},
   "source": [
    "In addition to using SMOTE for balancing the dataset, another effective approach is assigning class weights during model training. This technique helps the model pay more attention to the minority class (churn = 1) without altering the original dataset.\n",
    "\n",
    "Since our dataset is imbalanced, the model may naturally favor the majority class (churn = 0), leading to poor recall for the minority class. By assigning higher weights to the minority class, we encourage the model to give more importance to correctly predicting churn cases.\n",
    "\n",
    "In our case, we experimented with different class weight ratios (e.g., 1:1.3 and 1:1.5) and observed their impact on model performance. This helped improve the recall for churn cases while maintaining a balanced overall accuracy.\n",
    "\n",
    "Using class weights is especially useful in scenarios where oversampling techniques like SMOTE might introduce synthetic data artifacts, making it a valuable alternative for handling class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de86b69c-3440-4ac3-b277-510dbf7c81ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = {0: 1, 1: 1.4}  # 1 for non-churn, 1.5 for churn\n",
    "model.fit(X_train_scaled, y_train_resampled, class_weight=class_weight, epochs=50, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b46614c-df03-4551-99e0-d1070fd83c47",
   "metadata": {},
   "source": [
    "<h2>Model Evaluation</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dda6fa-af6e-4706-979d-b8fd4bf2b3d3",
   "metadata": {},
   "source": [
    "After training the model, it's crucial to evaluate its performance using multiple metrics, especially since we are dealing with an imbalanced dataset. Accuracy alone may be misleading, as the model could predict the majority class more often while failing to correctly identify churn cases.\n",
    "\n",
    "To get a comprehensive view of the model's performance, we use:\n",
    "\n",
    "<li>Precision – Measures how many predicted churn cases were actually correct.</li>\n",
    "<li>Recall – Measures how well the model identifies actual churn cases. Higher recall is important in churn prediction to minimize missed churners.</li>\n",
    "<li>F1-Score – Balances precision and recall, providing a better metric for imbalanced data.</li>\n",
    "<li>Confusion Matrix – Helps visualize the number of true positives, false positives, true negatives, and false negatives.</li>\n",
    "<br>\n",
    "Additionally, we experimented with threshold tuning (e.g., adjusting the default 0.5 threshold to 0.6) to optimize precision-recall trade-offs, ensuring a better balance between false positives and false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d233505a-9372-4865-9de8-723d8f1bdb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07874c0b-bb54-49ea-92e1-f589ff597e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_pred = (y_pred > 0.6)  # Convert probabilities to binary labels (0 or 1)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e68515-5ed1-47cc-8e7d-699407a4b5ec",
   "metadata": {},
   "source": [
    "<h2>Final Thoughts and Conclusion</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b111e598-12b1-436b-a6dc-8b41fdb0d633",
   "metadata": {},
   "source": [
    "After evaluating the model, we achieved a test accuracy of 87.41%, with a strong performance in predicting customer churn. The model effectively identifies at-risk customers, as shown by a recall of 63% for the churn class, meaning it captures most actual churn cases.\n",
    "\n",
    "Key takeaways from this project:\n",
    "<li>Addressed class imbalance using SMOTE and class weighting.</li>\n",
    "<li>Applied feature scaling for consistent model performance.</li>\n",
    "<li>Optimized hyperparameters and decision thresholds to improve recall.</li>\n",
    "<li>Evaluated performance using precision, recall, F1-score, and accuracy.</li>\n",
    "<br>\n",
    "Possible improvements:\n",
    "<li>Further hyperparameter tuning to optimize the precision-recall trade-off.</li>\n",
    "<li>Exploring additional feature engineering to extract more insights.</li>\n",
    "<li>Testing other architectures or models, such as tree-based algorithms or hybrid approaches.</li>\n",
    "<br>\n",
    "Overall, this project provides a solid deep learning-based approach to churn prediction, helping businesses take proactive steps to retain customers and reduce churn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba8696a-b545-4e1e-97a3-4df921e4979a",
   "metadata": {},
   "source": [
    "<hr>\n",
    "Thanks for your time.<br>\n",
    "I hope you found this notebook useful.<br>\n",
    "Author: Mohammadmehdi Omidi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a84ba3-925b-4ee3-ae00-cd1c18d34e02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
